1、逻辑回归与线性回归的联系与区别
1）线性回归要求变量服从正态分布，logistic回归对变量分布没有要求。 
2）线性回归要求因变量是连续性数值变量，而logistic回归要求因变量是分类型变量。 
3）线性回归要求自变量和因变量呈线性关系，而logistic回归不要求自变量和因变量呈线性关系 
4）logistic回归是分析因变量取某个值的概率与自变量的关系，而线性回归是直接分析因变量与自变量的关系

2、 逻辑回归的原理
逻辑回归是一个分类算法，它可以处理二元分类以及多元分类。虽然它名字里面有“回归”两个字，却不是一个回归算法。那为什么有“回归”这个误导性的词呢？虽然逻辑回归是分类模型，但是它的原理里面却残留着回归模型的影子，本文对逻辑回归原理做一个总结。
线性回归的模型是求出输出特征向量Y和输入样本矩阵X之间的线性关系系数θ，满足Y=Xθ。此时我们的Y是连续的，所以是回归模型。如果我们想要Y是离散的话，怎么办呢？一个可以想到的办法是，我们对于这个Y再做一次函数转换，变为g(Y)。如果我们令g(Y)的值在某个实数区间的时候是类别A，在另一个实数区间的时候是类别B，以此类推，就得到了一个分类模型。如果结果的类别只有两种，那么就是一个二元分类模型了。逻辑回归的出发点就是从这来的。下面我们开始引入二元逻辑回归。

3、逻辑回归损失函数推导及优化
1）构造预测函数 g(z)=1 /(1+e^(-Z))
2) 接下来需要确定数据划分的边界类型
3) 构造Cost函数
4) 梯度下降过程向量化

4、 正则化与模型评估指标
正则化
正则化是一种回归的形式，它将系数估计（coefficient estimate）朝零的方向进行约束、调整或缩小。也就是说，正则化可以在学习过程中降低模型复杂度和不稳定程度，从而避免过拟合的危险。
L1范数：L1范数在正则化的过程中会趋向于产生少量的特征，而其他的特征都是0（L1会使得参数矩阵变得稀疏）。因此L1不仅可以起到正则化的作用，还可以起到特征选择的作用。
L2范数：L2范数是通过使权重衰减，进而使得特征对于总体的影响减小而起到防止过拟合的作用的。L2的优点在于求解稳定、快速。
模型评估指标
精确率，precision = TP / (TP + FP) 即正确预测的正反例数 /总数
准确率，accuracy = (TP + TN) / (TP + FP + TN + FN) 精确率容易和准确率不能混为一谈，为预测出是正的里面有多少真正是正的。可理解为查准率。
召回率，recall = TP / (TP + FN) 表现为在实际正样本中，分类器能预测出多少。
F1 Score = P*R/2(P+R)，其中P和R分别为 precision 和 recall ，在precision与recall都要求高的情况下，可以用F1 Score来衡量。
ROC曲线 逻辑回归里面，对于正负例的界定，通常会设一个阈值，大于阈值的为正类，小于阈值为负类。如果我们减小这个阀值，更多的样本会被识别为正类，提高正类的识别率，但同时也会使得更多的负类被错误识别为正类。为了直观表示这一现象，引入ROC。在图中，横坐标为False Positive Rate(FPR假正率)，纵坐标为True Positive Rate(TPR真正率)。
AUC（Area Under Curve）被定义为ROC曲线下的面积(ROC的积分)，通常大于0.5小于1。AUC值(面积)越大的分类器，性能越好。

5、逻辑回归的优缺点
优点
1)形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那么这个特征最后对结果的影响会比较大。
2)模型效果不错，如果特征工程做的好，效果不会太差。
3)训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。

缺点
1)准确率并不是很高。因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布。
2)很难处理数据不平衡的问题。举个例子：如果我们对于一个正负样本非常不平衡的问题比如正负样本比 10000:1.我们把所有样本都预测为正也能使损失函数的值比较小。但是作为一个分类器，它对正负样本的区分能力不会很好。
3)处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据

6、样本不均衡问题解决办法
1) 增加数据
机器学习是使用现有的数据对整个数据的分布进行估计，因此更多的数据往往能够得到更多的分布信息，以及更好分布估计。即使再增加小类样本数据时，又增加了大类样本数据。

2) 尝试其它评价指标
在类别不均衡分类任务中，需要使用更有说服力的评价指标来对分类器进行评价。例如混淆矩阵,精确度,召回率,F1 Score,ROC曲线.

3) 对数据集进行重采样
对小类的数据样本进行采样来增加小类的数据样本个数，即过采样（over-sampling ，采样的个数大于该类样本的个数）。即添加部分样本的副本。
对大类的数据样本进行采样来减少该类数据样本的个数，即欠采样（under-sampling，采样的次数少于该类样本的个素）。即删除部分样本。

4) 尝试产生人工数据样本
一种简单的人工样本数据产生的方法便是，对该类下的所有样本每个属性特征的取值空间中随机选取一个组成新的样本，即属性值随机采样。有一个系统的构造人工数据样本的方法SMOTE(Synthetic Minority Over-sampling Technique)。SMOTE是一种过采样算法，它构造新的小类样本而不是产生小类中已有的样本的副本，即该算法构造的数据是新样本，原数据集中不存在的。

5) 尝试不同的分类算法
应该使用不同的算法对其进行比较，因为不同的算法使用于不同的任务与数据。决策树往往在类别不均衡数据上表现不错。它使用基于类变量的划分规则去创建分类树，因此可以强制地将不同类别的样本分开。

7、sklearn参数
1)penalty=‘l2’ : 字符串‘l1’或‘l2’,默认‘l2’。

用来指定惩罚的基准（正则化参数）。只有‘l2’支持‘newton-cg’、‘sag’和‘lbfgs’这三种算法。
如果选择‘l2’，solver参数可以选择‘liblinear’、‘newton-cg’、‘sag’和‘lbfgs’这四种算法；如果选择‘l1’的话就只能用‘liblinear’算法。

2)dual=False :
对偶或者原始方法。Dual只适用于正则化相为l2的‘liblinear’的情况，通常样本数大于特征数的情况下，默认为False。

3)C=1.0 : C为正则化系数λ的倒数，必须为正数，默认为1。和SVM中的C一样，值越小，代表正则化越强。

4)fit_intercept=True : 是否存在截距，默认存在。

5)intercept_scaling=1 : 仅在正则化项为‘liblinear’，且fit_intercept设置为True时有用。

6)solver=‘liblinear’ : solver参数决定了我们对逻辑回归损失函数的优化方法，有四种算法可以选择。
a) liblinear：使用了开源的liblinear库实现，内部使用了坐标轴下降法来迭代优化损失函数。 b) lbfgs：拟牛顿法的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。 c)newton-cg：也是牛顿法家族的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。 d) sag：即随机平均梯度下降，是梯度下降法的变种，和普通梯度下降法的区别是每次迭代仅仅用一部分的样本来计算梯度，适合于样本数据多的时候。
