# 线性回归算法梳理-1

[TOC]

## 1. 机器学习的一些概念

什么是机器学习？

（历史数据，结果）->算法模型

（未来数据）+算法模型->预测结果

推荐书目：机器学习实战

### 1.1 <u>应用背景</u>

(a)它会为你提供一个缩短编程时间的工具。EP：假设想编写一个程序来纠正拼写错误。我可以通过大量示例和经验法则（例如，I位于E之前，但出现在C之后时例外），取得一定的进展，然后经过数周的努力，编写出一个合理的程序。或者，我也可以使用现成的机器学习工具，只需向其提供一些样本，即可在很短的时间内获得一个更可靠的程序。

(b)借助机器学习，可以自定义产品，使其更适合特定的用户群体。假设我手动编写了一个英文拼写纠正程序，这个程序很成功，因此我打算针对100种最常用语言提供相应的版本。这样一来，每种语言版本几乎都需要从头开始，这将需要付出数年的努力。但如果我使用机器学习技术构建该程序，然后迁移到其他语言，基本上就相当于，我只需收集该特定语言的数据，并将这些数据提供给完全一样的机器学习模型即可。

(c)借助机器学习，帮助编程人员解决不知道如何用人工方法解决的问题。作为人类，我可以认出朋友的面孔，理解他们所说的话。但所有这些都是在潜意识下完成的，如果让我编写一个程序来做这些事，我会完全不知所措。但是，机器学习算法对此却很擅长，我不需要告诉算法应该怎么做，只需向其展示大量样本，问题就可以迎刃而解。

(d)可以改变思考问题的方式（哲学），像科学家一样思考。扩展视野，打开没有这项能力便无法探索的新世界的大门。所以，不妨享受这段旅程，愉快地探索其中的奥秘。

#### 1、一般应用

垃圾邮件分类、图像识别、人脸识别、数字识别

传统解决思路：

​    编写规则，定义“垃圾邮件”，让计算机执行：将一封邮件输入到传统算法，经判断输出结果；

​    弊端：对问题本身的规则很难定义；规则在不断变化；

#### 2、人类学习过程

　通过一定的样本资料，经过大脑的学习、归纳、整理、总结，获取知识和经验，在遇到类似的事务就可以根据经验和知识做出判断。

#### 3、机器学习过程

　对机器学习的算法，输入大量的学习资料，经过训练，得到一个可以以执行任务的算法（也称为模型）；在遇到新的样例，该模型可以做出判断。

#### 4、实例应用

　判断信用卡发放是否有风险、搜索引擎、电商平台的推荐系统、语音识别、人脸识别

　无人驾驶、安全领域、医疗领域、金融领域、市场领域、智能翻译

### 1.2 <u>机器学习与数据挖掘的区别</u>

机器语言是指在没有明确的程序指令的情况下，给予计算机学习能力，使它能自主的学习、设计和扩展相关算法。

数据挖掘则是一种从非结构化数据里面提取知识或者未知的、人们感兴趣的数据，在这个过程中应用了机器学习算法。

### 1.3 <u>机器学习流程</u>

机器学习是一个数据流转、分析以及得到结果的过程，它的整个流程大致可以分为六个步骤，安装数据流自上而下的顺序排列分别是：场景解析、数据预处理、特征工程、模型训练、模型评估、离线/在线服务。

（1）场景分析：就是先把整个业务逻辑想清楚，把自己的业务场景进行一个抽象；这里的场景抽象就是把业务逻辑和算法进行匹配。

（2）数据预处理：主要进行数据的清洗工作，该阶段的主要目标是减少量纲和噪音数据对于训练数据集的影响。

（3）特征工程：是机器学习中最重要的一个步骤，算法质量并不一定是决定结果的最关键因素，特征工程的效果从某种意义上决定了最终模型的优劣；也就是说，在算法相对固定的情况下，可以说好特征决定了好结果。

（4）模型训练：如下图所示的“逻辑回归二分类”组件表示的是算法训练过程，训练数据经过了数据预处理和特征工程之后进入算法训练模块，并且生成模型。

（5）模型评估：机器学习算法的计算结果一般是一个模型，模型的质量直接影响接下来的数据业务；对于模型的成熟度的评估，其实就是对整套机器学习流程的评估。

（6）离线/在线服务：在实际的业务运用过程中，机器学习通常需要配合调度系统来使用。

### 1.4 <u>数据源结构</u>

如果把机器学习算法比作一个数据加工场，那么进入工厂的数据就是被算法用来加工的原材料，机器学习算法需要的数据分为三类：结构化数据、非结构化数据和半结构化数据。

(1)结构化数据：是指我们在日常数据库处理中经常看到的日志类数据结构，是以矩阵结构存储在数据库中的数据，可以通过二维表结构来显示；结构化数据主要由两个部分组成，一个部分是每个字段的含义，另一个部分是每个字段的具体数值。

一般说来，机器学习算法处理的数据都是结构化的数据，因为机器学习需要把数据带入矩阵去做一些数学运算，结构化数据原生是以矩阵形态存储的，所以机器学习算法通常是只支持结构化数据的。

结构化数据中还有两个非常重要的概念：特征(Feature)和目标列 (Label)；其中特征表示的是数据所描述对象的属性，在结构化数据集中，每一列数据通常就对应一个特征；目标列表示的是每一份数据的打标结果。

(2)半结构化数据：是指按照一定的结构存储，但不是二维的数据库行存储形态的数据；另一种半结构化数据就是在数据表中，某些字段是文本型的，某些字段是数值型的。

半结构化数据常用于一些数据的传递，但是在机器学习算法相关的应用方面还有一定距离，需要把半结构化数据转为结构化数据来进行操作。

(3)非结构化数据：典型的非结构化数据就是图像、文本或者是语音文件，这些数据不能以矩阵的结构存储，目前的做法也是通过把非结构化数据转为二进制存储格式，然后通过算法来挖掘其中的信息。

### 1.5 <u>机器学习方法分类</u>

​	机械学习
​	示教学习
​	类比学习
​	归纳学习
​		监督学习

​               半监督学习		

​              非监督学习
​		      强化学习

（1）分类

**监督学习（Supervised Learning）**：输入数据有标签，即训练数据包含输入和预期的输出。

每个进入算法的训练数据样本都有对应的期望值也就是目标值，进行机器学习的过程实际上就是特征值和目标队列映射的过程；通过过往的一些数据的特征以及最终结果来进行训练的方式就是监督学习法；监督学习算法的训练数据源需要由特征值以及目标队列两部分组成。

因为监督学习依赖于每个样本的打标，可以得到每个特征序列映射到的确切的目标值是什么，所以常用于回归以及分类场景。常见的监督学习算法如下表所示：

| 算法     | 具体包括                                                |
| -------- | ------------------------------------------------------- |
| 分类算法 | K近邻、朴素贝叶斯、决策树、随机森林、GBDT和支持向量机等 |
| 回归算法 | 逻辑回归、线性回归等                                    |

**注：监督学习的一个问题就是获得目标值的成本比较高。**

**半监督学习（Semi-supervised Learning）**：学习器通过对有少量带标记的样本和大量未标记的样本的学习，建立模型用于预测未见样本的标记。（1先根据少量已标记的照片数据进行聚类，2再对未标记的照片根据1的分类结果进行聚类）

通过对样本的部分打标来进行机器学习算法的使用，很多半监督学习算法都是监督学习算法的变形。

![1565626259046](C:\Users\Tinker\AppData\Roaming\Typora\typora-user-images\1565626259046.png)

**非监督学习（Unsupervised Learning）**：输入数据没标签，即训练数据只有输入，没有预期的输出。

训练样本不依赖于打标数据的机器学习算法，它主要是用来解决一些聚类场景的问题。常见的无监督学习算法如下表所示：

| 算法     | 具体包括          |
| -------- | ----------------- |
| 聚类算法 | K-Means、DBSCAN等 |
| 推荐算法 | 协同过滤          |

**注：相较于监督学习，无监督学习的一大好处就是不依赖于打标数据。**

强化学习（Reinforcement Learning）：是机器学习中的一个领域，强调如何基于环境而行动，以取得最大化的预期利益。是一种比较复杂的机器学习种类， 强调的是系统与外界不断地交互，获得外界的反馈，然后决定自身的行为。

| 算法       | 具体包括                                           |
| ---------- | -------------------------------------------------- |
| 监督学习   | 逻辑回归、K 近邻、朴素贝叶斯、随机森林、支持向量机 |
| 半监督学习 | 标签传播                                           |
| 非监督学习 | K-means、DBSCAN、协同过滤、LDA                     |
| 强化学习   | 隐马尔可夫                                         |

### 1.6 <u>泛化能力：</u>

在机器学习方法中，泛化能力通俗来讲就是指学习到的模型对未知数据的预测能力。在实际情况中，我们通常通过测试误差来评价学习方法的泛化能力。如果在不考虑数据量不足的情况下出现模型的泛化能力差，那么其原因基本为对损失函数的优化没有达到全局最优。

### 1.7 <u>过拟合：</u>

在机器学习中，当一个统计模型首先描述随机误差或噪声，而不是自身的基本关系时，过度拟合就会出现。当一个模型是过于复杂，过拟合通常容易被发现，因为相对于训练数据类型的数量，参数的数量过于五花八门。那么这个模型由于过度拟合而效果不佳。举一个例子，在一个识别的任务当中，我们得到树叶的边缘是锯齿形的属性，这样在判断的过程中有锯齿形状属性会给树叶的识别增加一定的权重，当新的叶子没有锯齿形状的时候这个就很有可能被判断不是树叶，这样就导致了过度拟合。

从字面的意义上理解就是过度拟合的意思，常发生在线性分类器或者线性模型的训练和预测当中。过拟合的原理就是机器学习算法过度学习了训练集数据。 

如果在针对训练集做曲线拟合的时候做得过于“完美”，那么当我们针对于其他预测集进行预测的时候，这套模型很有可能会失准，因为这套模型在训练的时候过度地接近于训练集的特征，缺乏鲁棒性；所以在机器学习训练过程中，100%的拟合训练集数据并不一定是好的。

### 1.8 <u>欠拟合：</u>

指我们训练的模型要求过于宽泛无法达到我们预期的效果正确率低表达能力差。 

### 1.9 <u>交叉验证：</u>

有时亦称循环估计， 是一种统计学上将数据样本切割成较小子集的实用方法。于是可以先在一个子集上做分析， 而其它子集则用来做后续对此分析的确认及验证。 一开始的子集被称为训练集。而其它的子集则被称为验证集或测试集。交叉验证是一种评估统计分析、机器学习算法对独立于训练数据的数据集的泛化能力（generalize）。

交叉验证是在机器学习建立模型和验证模型参数时常用的办法。交叉验证，顾名思义，就是重复的使用数据，把得到的样本数据进行切分，组合为不同的训练集和测试集，用训练集来训练模型，用测试集来评估模型预测的好坏。在此基础上可以得到多组不同的训练集和测试集，某次训练集中的某样本在下次可能成为测试集中的样本，即所谓“交叉”。　

交叉验证一般要尽量满足：

1）训练集的比例要足够多，一般大于一半
2）训练集和测试集要均匀抽样

交叉验证主要分成以下几类：
**1）k-folder cross-validation:**
k个子集，每个子集均做一次测试集，其余的作为训练集。交叉验证重复k次，每次选择一个子集作为测试集，并将k次的平均交叉验证识别正确率作为结果。
优点：所有的样本都被作为了训练集和测试集，每个样本都被验证一次。10-folder通常被使用。
**2）K \* 2 folder cross-validation**
是k-folder cross-validation的一个变体，对每一个folder，都平均分成两个集合s0,s1，我们先在集合s0训练用s1测试，然后用s1训练s0测试。
优点是：测试和训练集都足够大，每一个个样本都被作为训练集和测试集。一般使用k=10
**3)least-one-out cross-validation(loocv)**
假设dataset中有n个样本，那LOOCV也就是n-CV，意思是每个样本单独作为一次测试集，剩余n-1个样本则做为训练集。
优点：

## 1）每一回合中几乎所有的样本皆用于训练model，因此最接近母体样本的分布，估测所得的generalization error比较可靠。

2）实验过程中没有随机因素会影响实验数据，确保实验过程是可以被复制的。
但LOOCV的缺点则是计算成本高，为需要建立的models数量与总样本数量相同，当总样本数量相当多时，LOOCV在实作上便有困难，除非每次训练model的速度很快，或是可以用平行化计算减少计算所需的时间。

-------**十折交叉验证：10-fold cross validation**----------

英文名叫做10-fold cross-validation，用来测试算法准确性。是常用的测试方法。将数据集分成十分，轮流将其中9份作为训练数据，1份作为测试数据，进行试验。每次试验都会得出相应的正确率（或差错率）。10次的结果的正确率（或差错率）的平均值作为对算法精度的估计，一般还需要进行多次10折交叉验证（例如10次10折交叉验证），再求其均值，作为对算法准确性的估计。

之所以选择将数据集分为10份，是因为通过利用大量数据集、使用不同学习技术进行的大量试验，表明10折是获得最好误差估计的恰当选择，而且也有一些理论根据可以证明这一点。但这并非最终诊断，争议仍然存在。而且似乎5折或者20折与10折所得出的结果也相差无几。

## 2. 线性回归的原理

我们使用线性回归是在这堆数据所在的N维空间中找到一条线来描述这些数据的规律，因此才叫线性回归。这个过程称为拟合，这条线成为拟合线。

### 2.1 线性回归的模型函数

线性回归遇到的问题一般是这样的。我们有m个样本，每个样本对应于n维特征和一个结果输出，如下：

　　　　\((x_1^{(0)}, x_2^{(0)}, ...x_n^{(0)}, y_0), (x_1^{(1)}, x_2^{(1)}, ...x_n^{(1)},y_1), ... (x_1^{(m)}, x_2^{(m)}, ...x_n^{(m)}, y_m)\)

　　　　我们的问题是，对于一个新的\((x_1^{(x)}, x_2^{(x)}, ...x_n^{(x)} \), 他所对应的\(y_x\)是多少呢？ 如果这个问题里面的y是连续的，则是一个回归问题，否则是一个分类问题。

　　　　对于n维特征的样本数据，如果我们决定使用线性回归，那么对应的模型是这样的：

　　　　\(h_\theta(x_1, x_2, ...x_n) = \theta_0 + \theta_{1}x_1 + ... + \theta_{n}x_{n}\), 其中\(\theta_i \) (i = 0,1,2... n)为模型参数，\(x_i \) (i = 0,1,2... n)为每个样本的n个特征值。这个表示可以简化，我们增加一个特征\(x_0 = 1 \) ，这样\(h_\theta(x_0, x_1, ...x_n) = \sum\limits_{i=0}^{n}\theta_{i}x_{i}\)。

　　　　进一步用矩阵形式表达更加简洁如下：

　　　　\(h_\mathbf{\theta}(\mathbf{X}) = \mathbf{X\theta}\)

　　　　其中， 假设函数\(h_\mathbf{\theta}(\mathbf{X})\)为mx1的向量,\(\mathbf{\theta}\)为nx1的向量，里面有n个代数法的模型参数。\(\mathbf{X}\)为mxn维的矩阵。m代表样本的个数，n代表样本的特征数。

得到了模型，我们需要求出需要的损失函数，一般线性回归我们用均方误差作为损失函数。损失函数的代数法表示如下：

　　　　\(J(\theta_0, \theta_1..., \theta_n) = \sum\limits_{i=1}^{m}(h_\theta(x_0^{(i)}, x_1^{(i)}, ...x_n^{(i)}) - y_i)^2\)

　　　　进一步用矩阵形式表达损失函数：

　　　　\(J(\mathbf\theta) = \frac{1}{2}(\mathbf{X\theta} - \mathbf{Y})^T(\mathbf{X\theta} - \mathbf{Y})\)

　　　　由于矩阵法表达比较的简洁，后面我们将统一采用矩阵方式表达模型函数和损失函数。

## 3. 线性回归损失函数、代价函数、目标函数

损失函数（Loss Function ）是定义在单个样本上的，算的是一个样本的误差。

代价函数（Cost Function ）是定义在整个训练集上的，是所有样本误差的平均，也就是损失函数的平均。

目标函数（Object Function）定义为：最终需要优化的函数。等于经验风险+结构风险（也就是Cost Function + 正则化项）。

## 4. 优化方法

梯度下降法、最小二乘法、牛顿法、拟牛顿法

对于线性回归的损失函数\(J(\mathbf\theta) = \frac{1}{2}(\mathbf{X\theta} - \mathbf{Y})^T(\mathbf{X\theta} - \mathbf{Y})\)，我们常用的有两种方法来求损失函数最小化时候的\(\mathbf{\theta}\)参数：一种是**梯度下降法**，一种是**最小二乘法**。由于已经在其它篇中单独介绍了梯度下降法和最小二乘法，可以点链接到对应的文章链接去阅读。

　　　　如果采用梯度下降法，则\(\mathbf{\theta}\)的迭代公式是这样的：

　　　　\(\mathbf\theta= \mathbf\theta - \alpha\mathbf{X}^T(\mathbf{X\theta} - \mathbf{Y})\)

　　　　通过若干次迭代后，我们可以得到最终的\(\mathbf{\theta}\)的结果

　　　　如果采用最小二乘法，则\(\mathbf{\theta}\)的结果公式如下：

　　　　\( \mathbf{\theta} = (\mathbf{X^{T}X})^{-1}\mathbf{X^{T}Y} \)

 

　　　　当然线性回归，还有其他的常用算法，比如牛顿法和拟牛顿法，这里不详细描述。

## 5. 线性回归的评估指标

评价线性回归的指标有四种，均方误差（Mean Squared Error）、均方根误差（Root Mean Squared Error）、平均绝对值误差（Mean Absolute Error）以及R Squared方法。 sklearnz中使用的，也是大家推荐的方法是R Squared方法。





