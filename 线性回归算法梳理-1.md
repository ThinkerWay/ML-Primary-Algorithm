# 线性回归算法梳理-1

[TOC]

## 1. 机器学习的一些概念

什么是机器学习？

（历史数据，结果）->算法模型

（未来数据）+算法模型->预测结果

推荐书目：机器学习实战

### 1.1 <u>应用背景</u>

(a)它会为你提供一个缩短编程时间的工具。EP：假设想编写一个程序来纠正拼写错误。我可以通过大量示例和经验法则（例如，I位于E之前，但出现在C之后时例外），取得一定的进展，然后经过数周的努力，编写出一个合理的程序。或者，我也可以使用现成的机器学习工具，只需向其提供一些样本，即可在很短的时间内获得一个更可靠的程序。

(b)借助机器学习，可以自定义产品，使其更适合特定的用户群体。假设我手动编写了一个英文拼写纠正程序，这个程序很成功，因此我打算针对100种最常用语言提供相应的版本。这样一来，每种语言版本几乎都需要从头开始，这将需要付出数年的努力。但如果我使用机器学习技术构建该程序，然后迁移到其他语言，基本上就相当于，我只需收集该特定语言的数据，并将这些数据提供给完全一样的机器学习模型即可。

(c)借助机器学习，帮助编程人员解决不知道如何用人工方法解决的问题。作为人类，我可以认出朋友的面孔，理解他们所说的话。但所有这些都是在潜意识下完成的，如果让我编写一个程序来做这些事，我会完全不知所措。但是，机器学习算法对此却很擅长，我不需要告诉算法应该怎么做，只需向其展示大量样本，问题就可以迎刃而解。

(d)可以改变思考问题的方式（哲学），像科学家一样思考。扩展视野，打开没有这项能力便无法探索的新世界的大门。所以，不妨享受这段旅程，愉快地探索其中的奥秘。

#### 1、一般应用

垃圾邮件分类、图像识别、人脸识别、数字识别

传统解决思路：

​    编写规则，定义“垃圾邮件”，让计算机执行：将一封邮件输入到传统算法，经判断输出结果；

​    弊端：对问题本身的规则很难定义；规则在不断变化；

#### 2、人类学习过程

　通过一定的样本资料，经过大脑的学习、归纳、整理、总结，获取知识和经验，在遇到类似的事务就可以根据经验和知识做出判断。

#### 3、机器学习过程

　对机器学习的算法，输入大量的学习资料，经过训练，得到一个可以以执行任务的算法（也称为模型）；在遇到新的样例，该模型可以做出判断。

#### 4、实例应用

　判断信用卡发放是否有风险、搜索引擎、电商平台的推荐系统、语音识别、人脸识别

　无人驾驶、安全领域、医疗领域、金融领域、市场领域、智能翻译

### 1.2 <u>机器学习与数据挖掘的区别</u>

机器语言是指在没有明确的程序指令的情况下，给予计算机学习能力，使它能自主的学习、设计和扩展相关算法。

数据挖掘则是一种从非结构化数据里面提取知识或者未知的、人们感兴趣的数据，在这个过程中应用了机器学习算法。

### 1.3 <u>机器学习流程</u>

机器学习是一个数据流转、分析以及得到结果的过程，它的整个流程大致可以分为六个步骤，安装数据流自上而下的顺序排列分别是：场景解析、数据预处理、特征工程、模型训练、模型评估、离线/在线服务。

（1）场景分析：就是先把整个业务逻辑想清楚，把自己的业务场景进行一个抽象；这里的场景抽象就是把业务逻辑和算法进行匹配。

（2）数据预处理：主要进行数据的清洗工作，该阶段的主要目标是减少量纲和噪音数据对于训练数据集的影响。

（3）特征工程：是机器学习中最重要的一个步骤，算法质量并不一定是决定结果的最关键因素，特征工程的效果从某种意义上决定了最终模型的优劣；也就是说，在算法相对固定的情况下，可以说好特征决定了好结果。

（4）模型训练：如下图所示的“逻辑回归二分类”组件表示的是算法训练过程，训练数据经过了数据预处理和特征工程之后进入算法训练模块，并且生成模型。

（5）模型评估：机器学习算法的计算结果一般是一个模型，模型的质量直接影响接下来的数据业务；对于模型的成熟度的评估，其实就是对整套机器学习流程的评估。

（6）离线/在线服务：在实际的业务运用过程中，机器学习通常需要配合调度系统来使用。

### 1.4 <u>数据源结构</u>

如果把机器学习算法比作一个数据加工场，那么进入工厂的数据就是被算法用来加工的原材料，机器学习算法需要的数据分为三类：结构化数据、非结构化数据和半结构化数据。

(1)结构化数据：是指我们在日常数据库处理中经常看到的日志类数据结构，是以矩阵结构存储在数据库中的数据，可以通过二维表结构来显示；结构化数据主要由两个部分组成，一个部分是每个字段的含义，另一个部分是每个字段的具体数值。

一般说来，机器学习算法处理的数据都是结构化的数据，因为机器学习需要把数据带入矩阵去做一些数学运算，结构化数据原生是以矩阵形态存储的，所以机器学习算法通常是只支持结构化数据的。

结构化数据中还有两个非常重要的概念：特征(Feature)和目标列 (Label)；其中特征表示的是数据所描述对象的属性，在结构化数据集中，每一列数据通常就对应一个特征；目标列表示的是每一份数据的打标结果。

(2)半结构化数据：是指按照一定的结构存储，但不是二维的数据库行存储形态的数据；另一种半结构化数据就是在数据表中，某些字段是文本型的，某些字段是数值型的。

半结构化数据常用于一些数据的传递，但是在机器学习算法相关的应用方面还有一定距离，需要把半结构化数据转为结构化数据来进行操作。

(3)非结构化数据：典型的非结构化数据就是图像、文本或者是语音文件，这些数据不能以矩阵的结构存储，目前的做法也是通过把非结构化数据转为二进制存储格式，然后通过算法来挖掘其中的信息。

### 1.5 <u>机器学习方法分类</u>

​	机械学习
​	示教学习
​	类比学习
​	归纳学习
​		监督学习

​               半监督学习		

​              非监督学习
​		      强化学习

（1）分类

**监督学习（Supervised Learning）**：输入数据有标签，即训练数据包含输入和预期的输出。

每个进入算法的训练数据样本都有对应的期望值也就是目标值，进行机器学习的过程实际上就是特征值和目标队列映射的过程；通过过往的一些数据的特征以及最终结果来进行训练的方式就是监督学习法；监督学习算法的训练数据源需要由特征值以及目标队列两部分组成。

因为监督学习依赖于每个样本的打标，可以得到每个特征序列映射到的确切的目标值是什么，所以常用于回归以及分类场景。常见的监督学习算法如下表所示：

| 算法     | 具体包括                                                |
| -------- | ------------------------------------------------------- |
| 分类算法 | K近邻、朴素贝叶斯、决策树、随机森林、GBDT和支持向量机等 |
| 回归算法 | 逻辑回归、线性回归等                                    |

**注：监督学习的一个问题就是获得目标值的成本比较高。**

**半监督学习（Semi-supervised Learning）**：学习器通过对有少量带标记的样本和大量未标记的样本的学习，建立模型用于预测未见样本的标记。（1先根据少量已标记的照片数据进行聚类，2再对未标记的照片根据1的分类结果进行聚类）

通过对样本的部分打标来进行机器学习算法的使用，很多半监督学习算法都是监督学习算法的变形。

![1565626259046](C:\Users\Tinker\AppData\Roaming\Typora\typora-user-images\1565626259046.png)

**非监督学习（Unsupervised Learning）**：输入数据没标签，即训练数据只有输入，没有预期的输出。

训练样本不依赖于打标数据的机器学习算法，它主要是用来解决一些聚类场景的问题。常见的无监督学习算法如下表所示：

| 算法     | 具体包括          |
| -------- | ----------------- |
| 聚类算法 | K-Means、DBSCAN等 |
| 推荐算法 | 协同过滤          |

**注：相较于监督学习，无监督学习的一大好处就是不依赖于打标数据。**

强化学习（Reinforcement Learning）：是机器学习中的一个领域，强调如何基于环境而行动，以取得最大化的预期利益。是一种比较复杂的机器学习种类， 强调的是系统与外界不断地交互，获得外界的反馈，然后决定自身的行为。

| 算法       | 具体包括                                           |
| ---------- | -------------------------------------------------- |
| 监督学习   | 逻辑回归、K 近邻、朴素贝叶斯、随机森林、支持向量机 |
| 半监督学习 | 标签传播                                           |
| 非监督学习 | K-means、DBSCAN、协同过滤、LDA                     |
| 强化学习   | 隐马尔可夫                                         |

### 1.6 <u>泛化能力：</u>

在机器学习方法中，泛化能力通俗来讲就是指学习到的模型对未知数据的预测能力。在实际情况中，我们通常通过测试误差来评价学习方法的泛化能力。如果在不考虑数据量不足的情况下出现模型的泛化能力差，那么其原因基本为对损失函数的优化没有达到全局最优。

### 1.7 <u>过拟合：</u>

在机器学习中，当一个统计模型首先描述随机误差或噪声，而不是自身的基本关系时，过度拟合就会出现。当一个模型是过于复杂，过拟合通常容易被发现，因为相对于训练数据类型的数量，参数的数量过于五花八门。那么这个模型由于过度拟合而效果不佳。举一个例子，在一个识别的任务当中，我们得到树叶的边缘是锯齿形的属性，这样在判断的过程中有锯齿形状属性会给树叶的识别增加一定的权重，当新的叶子没有锯齿形状的时候这个就很有可能被判断不是树叶，这样就导致了过度拟合。

从字面的意义上理解就是过度拟合的意思，常发生在线性分类器或者线性模型的训练和预测当中。过拟合的原理就是机器学习算法过度学习了训练集数据。 

如果在针对训练集做曲线拟合的时候做得过于“完美”，那么当我们针对于其他预测集进行预测的时候，这套模型很有可能会失准，因为这套模型在训练的时候过度地接近于训练集的特征，缺乏鲁棒性；所以在机器学习训练过程中，100%的拟合训练集数据并不一定是好的。

### 1.8 <u>欠拟合：</u>

指我们训练的模型要求过于宽泛无法达到我们预期的效果正确率低表达能力差。 

### 1.9 <u>交叉验证：</u>

有时亦称循环估计， 是一种统计学上将数据样本切割成较小子集的实用方法。于是可以先在一个子集上做分析， 而其它子集则用来做后续对此分析的确认及验证。 一开始的子集被称为训练集。而其它的子集则被称为验证集或测试集。交叉验证是一种评估统计分析、机器学习算法对独立于训练数据的数据集的泛化能力（generalize）。

交叉验证是在机器学习建立模型和验证模型参数时常用的办法。交叉验证，顾名思义，就是重复的使用数据，把得到的样本数据进行切分，组合为不同的训练集和测试集，用训练集来训练模型，用测试集来评估模型预测的好坏。在此基础上可以得到多组不同的训练集和测试集，某次训练集中的某样本在下次可能成为测试集中的样本，即所谓“交叉”。　

交叉验证一般要尽量满足：

1）训练集的比例要足够多，一般大于一半
2）训练集和测试集要均匀抽样

交叉验证主要分成以下几类：
**1）k-folder cross-validation:**
k个子集，每个子集均做一次测试集，其余的作为训练集。交叉验证重复k次，每次选择一个子集作为测试集，并将k次的平均交叉验证识别正确率作为结果。
优点：所有的样本都被作为了训练集和测试集，每个样本都被验证一次。10-folder通常被使用。
**2）K \* 2 folder cross-validation**
是k-folder cross-validation的一个变体，对每一个folder，都平均分成两个集合s0,s1，我们先在集合s0训练用s1测试，然后用s1训练s0测试。
优点是：测试和训练集都足够大，每一个个样本都被作为训练集和测试集。一般使用k=10
**3)least-one-out cross-validation(loocv)**
假设dataset中有n个样本，那LOOCV也就是n-CV，意思是每个样本单独作为一次测试集，剩余n-1个样本则做为训练集。
优点：

1）每一回合中几乎所有的样本皆用于训练model，因此最接近母体样本的分布，估测所得的generalization error比较可靠。

2）实验过程中没有随机因素会影响实验数据，确保实验过程是可以被复制的。
但LOOCV的缺点则是计算成本高，为需要建立的models数量与总样本数量相同，当总样本数量相当多时，LOOCV在实作上便有困难，除非每次训练model的速度很快，或是可以用平行化计算减少计算所需的时间。

-------**十折交叉验证：10-fold cross validation**----------

英文名叫做10-fold cross-validation，用来测试算法准确性。是常用的测试方法。将数据集分成十分，轮流将其中9份作为训练数据，1份作为测试数据，进行试验。每次试验都会得出相应的正确率（或差错率）。10次的结果的正确率（或差错率）的平均值作为对算法精度的估计，一般还需要进行多次10折交叉验证（例如10次10折交叉验证），再求其均值，作为对算法准确性的估计。

之所以选择将数据集分为10份，是因为通过利用大量数据集、使用不同学习技术进行的大量试验，表明10折是获得最好误差估计的恰当选择，而且也有一些理论根据可以证明这一点。但这并非最终诊断，争议仍然存在。而且似乎5折或者20折与10折所得出的结果也相差无几。

## 2. 线性回归的原理

我们使用线性回归是在这堆数据所在的N维空间中找到一条线来描述这些数据的规律，因此才叫线性回归。这个过程称为拟合，这条线成为拟合线。

### 2.1 线性回归的模型函数

​       线性回归遇到的问题一般是这样的。我们有m个样本，每个样本对应于n维特征和一个结果输出，如下：
$$
((x_1^{(0)}, x_2^{(0)}, ...x_n^{(0)}, y_0), (x_1^{(1)}, x_2^{(1)}, ...x_n^{(1)},y_1), ... (x_1^{(m)}, x_2^{(m)}, ...x_n^{(m)}, y_m))
$$
​        对于一个新的$((x_1^{(x)}, x_2^{(x)}, ...x_n^{(x)})$ , 他所对应的\(y_x\)是多少呢？ 如果这个问题里面的y是连续的，则是一个回归问题，否则是一个分类问题。

　　对于n维特征的样本数据，如果我们决定使用线性回归，那么对应的模型是这样的：
$$
h_\theta(x_1, x_2, ...x_n) = \theta_0 + \theta_{1}x_1 + ... + \theta_{n}x_{n}
$$
，其中$\theta_i (i = 0,1,2... n)$为模型参数，$x_i (i = 0,1,2... n)$为每个样本的n个特征值。这个表示可以简化，我们增加一个特征$x_0 = 1$ ，这样$h_\theta(x_0, x_1, ...x_n) = \sum\limits_{i=0}^{n}\theta_{i}x_{i}$。

　　进一步用矩阵形式表达更加简洁如下：
$$
h_\mathbf{\theta}(\mathbf{X}) = \mathbf{X\theta}
$$
​       其中， 假设函数$h_\mathbf{\theta}(\mathbf{X})$为$m\times1$的向量,$\mathbf{\theta}$为$(n+1)\times1$的向量，里面有$n+1$个代数法的模型参数。$\mathbf{X}$为$m\times(n+1)$维的矩阵，前$n$维数据是数据集的数据，最后一维值为1，代表$x_0$。$m$代表样本的个数，$n$代表样本的特征数。
$$
\mathbf{X}=
\left(
\begin{matrix}
x_{11} & x_{12} & \cdots & x_{1n} & 1 \\
x_{21} & x_{22} & \cdots & x_{2n} & 1 \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
x_{m1} & x_{m2} & \cdots & x_{mn} & 1 \\
\end{matrix}
\right)
$$
​      所以，求解线性模型的解，就是求解$\mathbf{\theta}$。

得到了模型，我们需要求出需要的损失函数，一般线性回归我们用均方误差作为损失函数。损失函数的代数法表示如下：
$$
J(\theta_0, \theta_1..., \theta_n) = \sum\limits_{i=1}^{m}(h_\theta(x_0^{(i)}, x_1^{(i)}, ...x_n^{(i)}) - y_i)^2
$$
　　进一步用矩阵形式表达损失函数：
$$
J(\mathbf\theta) = \frac{1}{2}(\mathbf{X\theta} - \mathbf{Y})^T(\mathbf{X\theta} - \mathbf{Y})
$$
​        由于矩阵法表达比较的简洁，后面我们将统一采用矩阵方式表达模型函数和损失函数。当误差函数最小时，有最优解。 求解方法有梯度下降法（Gradient Descent）、Normal Equation等等。

注：**梯度下降法**有如下特点：需要预先选定步长a、需要多次迭代、特征值需要Scaling（统一到同一个尺度范围），因此比较复杂。还有一种不需要迭代的求解方式-**Normal Equation**，简单、方便、不需要Feature Scaling。但是Normal Equation**最小二乘法**需要计算X的转置与逆矩阵，计算量很大，因此特征个数多时计算会很慢，只适用于特征个数小于100000时使用；当特征数量大于100000时使用梯度法。另外，当$\mathbf{X}$不可逆时就有**岭回归算法**的用武之地了。

## 3. 线性回归损失函数、代价函数、目标函数

损失函数（Loss Function ）是定义在单个样本上的，算的是一个样本的误差。

代价函数（Cost Function ）是定义在整个训练集上的，是所有样本误差的平均，也就是损失函数的平均。

目标函数（Object Function）定义为：最终需要优化的函数。等于经验风险+结构风险（也就是Cost Function + 正则化项）。

## 4. 优化方法

梯度下降法、最小二乘法、牛顿法、拟牛顿法

### 4.1 Gradient Descent-梯度下降法

依据平方误差。定义该线性回归模型的损耗/代价函数（Cost Function）为：
$$
J(\theta) = J(\theta_0, \theta_1,...,\theta_n) = \frac{1}{2m}\sum_{i=1}^{n}(h_{\theta}(x^{(i)})-y^{(i)})^2
$$
线性回归的损耗/代价函数的值与回归系数$\theta$的关系是碗状的。仅仅有一个最小点。线性回归的求解过程如同Logistic回归，差别在于学习模型函数$h_{\theta}(x)$不同，梯度法具体求解过程參考“[机器学习经典算法具体解释及Python实现---Logistic回归（LR）分类器](http://blog.csdn.net/suipingsp/article/details/41822313)”。

### 4.2 Normal Equation-普通最小二乘法

Normal Equation算法也叫做普通最小二乘法（ordinary least squares），其特点是：给定输人矩阵$\mathbf{X}$，假设$\mathbf{X}^T\mathbf{X}$的逆存在并能够求得的话。就能够直接採用该方法求解。

其求解理论也十分简单：既然是是求最小误差平方和。另其导数为0就可以得出回归系数。
$$
\theta^*=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}
$$
矩阵$\mathbf{X}$为$(m，n+1)$矩阵（m表示样本数、n表示一个样本的特征数），$y$为$(m,1)$列向量。 

上述公式中包括$\mathbf{X}^T\mathbf{X}$, 也就是须要对矩阵求逆，因此这个方程仅仅在逆矩阵存在的时候适用。然而。矩阵的逆可能并不存在，后面“岭回归”会讨论处理方法。

### 4.3 Newton‘s Method-牛顿法

​       **牛顿法**（**Newton'smethod**）又称为**牛顿-拉弗森方法**（**Newton-Raphson method**），它是一种在实数域和复数域上近似求解方程的方法。

**泰勒公式**：对于函数，如果函数平滑且某点存在各阶导数，则可以用一个多项式来描述该点邻域的近似值。公式如下：
$$
f(x)=\sum_{n=0}^{+\infty}f^{(n)}(x_0)\times\frac{1}{n!}(x-x_0)^n
$$
​        牛顿法是为了求解函数值为零的时候变量的取值问题的。数值优化算法除了梯度下降法外还有比较常用的一种方法是牛顿法。==对于非线性方程，可以用牛顿迭代法进行求解，它收敛速度快。==

​        基本思想：对于非线性函数$f(x)$，根据泰勒公式得到的$x$附近某个点$x_k$展开的多项式可用来近似函数$f(x)$的值，该多项式对应的函数为$F(x)$，求得$F(x)$的极小值作为新的迭代点，然后继续在新的迭代点泰勒公式展开，直到求得的极小值满足一定的精度。

**原理**

假设函数$f(x)$二次可微，则二次泰勒展开，
$$
f(x)\approx g(x)=f(x_k)+f^{'}(x_k)(x-x_k)+\frac{1}{2}f^{''}(x_k)(x-x_k)^2
$$
$g(x)$多项式则为$f(x)$的近似，求函数$f(x)$极值则可转化为求导函数为0，对$g(x)$求导并另其为0，
$$
f^{'}(x_k)+f^{''}(x_k)(x-x_k)=0
$$
得到，
$$
x=x_k-\frac{f^{'}(x_k)}{f^{''}(x_k)}
$$
即得到迭代公式，
$$
x_{k+1}=x_k-\frac{f^{'}(x_k)}{f^{''}(x_k)}
$$
新的点$x_{k+1}$不断逼近极值，直到一次导数小于某误差。

**迭代步骤**

1）确定初始点$x_0$，确定误差大小$e$（精度要求）。

2）计算$f^{'}(x_k)$，若它的绝对值小于$e$则停止迭代，$x_k$即为极值点。

3）计算$f^{''}(x_k)$，并根据迭代公式求得$x_{k+1}$。

4）跳转到步骤2）。

**实现代码**

```python
def f(x):
    #待完善
```

​        **牛顿法求最优值的步骤：**

1）随机选取起始点$x_0$；

2）计算目标函数$f(x)$在该点$x_k$的一阶导数和海森矩阵；

3）依据迭代公式更新x值

如果E(f(xk+1)−f(xk))<ϵ，则收敛返回，否则继续步骤2,3直至收敛。

### 4.4 拟牛顿法

当我们的特征特别多的时候，求海森矩阵的逆的运算量是非常大且慢的，这对于在实际应用中是不可忍受的，因此我们想能否用一个矩阵来代替海森矩阵的逆呢，这就是拟牛顿法的基本思路。具体算法可参考https://www.baidu.com/link?url=lqiY-buBqS8DdrY8Gl-a4F-MgU1EoxMJV9iN0eK_1VEVquTE8Ce-Qu2ylPr5tz40WJ307NimnSQ6U1w0BRQbBG2zBt9O-bRx-Vhj1hSr8Ym&wd=&eqid=a406f0780003954c000000065d557c80。

对于线性回归的损失函数$J(\mathbf\theta) = \frac{1}{2}(\mathbf{X\theta} - \mathbf{Y})^T(\mathbf{X\theta} - \mathbf{Y})$，我们常用的有两种方法来求损失函数最小化时候的$\mathbf{\theta}$参数：一种是**梯度下降法**，一种是**最小二乘法**。由于已经在其它篇中单独介绍了梯度下降法和最小二乘法，可以点链接到对应的文章链接去阅读。

​       如果采用**梯度下降法**，则$\mathbf{\theta}$的迭代公式是这样的：
$$
\mathbf\theta= \mathbf\theta - \alpha\mathbf{X}^T(\mathbf{X\theta} - \mathbf{Y})
$$
　　通过若干次迭代后，我们可以得到最终的$\mathbf{\theta}$的结果

　　如果采用**最小二乘法**，则$\mathbf{\theta}$的结果公式如下：
$$
\mathbf{\theta} = (\mathbf{X^{T}X})^{-1}\mathbf{X^{T}Y}
$$
　　当然线性回归，还有其他的常用算法，比如牛顿法和拟牛顿法，这里不详细描述。

## 5. 线性回归的评估指标

评价线性回归的指标有四种，均方误差（Mean Squared Error）、均方根误差（Root Mean Squared Error）、平均绝对值误差（Mean Absolute Error）以及R Squared方法。 sklearnz中使用的，也是大家推荐的方法是R Squared方法。

## 6.sklearn参数详解

网上有很多关于sklearn的学习教程，大部分都是简单的讲清楚某一方面，其实最好的教程就是官方文档。

　　官方文档地址：https://scikit-learn.org/stable/

　　自2007年发布以来，scikit-learn已经成为Python重要的机器学习库了，scikit-learn简称sklearn，支持包括分类，回归，降维和聚类四大机器学习算法。还包括了特征提取，数据处理和模型评估者三大模块。

　　sklearn是Scipy的扩展，建立在Numpy和matplolib库的基础上。利用这几大模块的优势，可以大大的提高机器学习的效率。

　　sklearn拥有着完善的文档，上手容易，具有着丰富的API，在学术界颇受欢迎。sklearn已经封装了大量的机器学习算法，包括LIBSVM和LIBINEAR。同时sklearn内置了大量数据集，节省了获取和整理数据集的时间。

### 6.1 KNN

•n_neighbors：默认为5，就是k-NN的k的值，选取最近的k个点。

　　        •weights：默认是uniform，参数可以是uniform、distance，也可以是用户自己定义的函数。uniform是均等的权重，就说所有的邻近点的权重都是相等的。distance是不均等的权重，距离近的点比距离远的点的影响大。用户自定义的函数，接收距离的数组，返回一组维数相同的权重。

　　        •algorithm：快速k近邻搜索算法，默认参数为auto，可以理解为算法自己决定合适的搜索算法。除此之外，用户也可以自己指定搜索算法ball_tree、kd_tree、brute方法进行搜索，brute是蛮力搜索，也就是线性扫描，当训练集很大时，计算非常耗时。kd_tree，构造kd树存储数据以便对其进行快速检索的树形数据结构，kd树也就是数据结构中的二叉树。以中值切分构造的树，每个结点是一个超矩形，在维数小于20时效率高。balltree是为了克服kd树高纬失效而发明的，其构造过程是以质心C和半径r分割样本空间，每个节点是一个超球体。

　　        •leaf_size：默认是30，这个是构造的kd树和ball树的大小。这个值的设置会影响树构建的速度和搜索速度，同样也影响着存储树所需的内存大小。需要根据问题的性质选择最优的大小。

　　        •metric：用于距离度量，默认度量是minkowski，也就是p=2的欧氏距离(欧几里德度量)。

　　        •p：距离度量公式。在上小结，我们使用欧氏距离公式进行距离度量。除此之外，还有其他的度量方法，例如曼哈顿距离。这个参数默认为2，也就是默认使用欧式距离公式进行距离度量。也可以设置为1，使用曼哈顿距离公式进行距离度量。

　　        •metric_params：距离公式的其他关键参数，这个可以不管，使用默认的None即可。

　　        •n_jobs：并行处理设置。默认为1，临近点搜索并行工作数。如果为-1，那么CPU的所有cores都用于并行工作。

### 6.2 Kmeans

•n_clusters:簇的个数，即你想聚成几类

　　        •init:初始簇中心的获取方法

　　        •n_init:获取初始簇中心的更迭次数，为了弥补初始质心的影响，算法默认会初始10次质心，实现算法，然后返回最好的结果。

　　        •max_iter:最大迭代次数（因为kmeans算法的实现需要迭代）

　　        •tol:容忍度，即kmeans运行准则收敛的条件

　　        •precompute_distances：是否需要提前计算距离，这个参数会在空间和时间之间做权衡，如果是True会把整个距离矩阵都放到内存中，auto会默认在数据样本大于featurs*samples的数量大于12e6的时候False,False时核心实现的方法是利用Cpython来实现的

　　        •verbose:冗长模式（不太懂是啥意思，反正一般不去改默认值）

　　        •random_state:随机生成簇中心的状态条件。

　　        •copy_x:对是否修改数据的一个标记，如果True，即复制了就不会修改数据。bool在scikit-learn很多接口中都会有这个参数的，就是是否对输入数据继续copy操作，以便不修改用户的输入数据。这个要理解Python的内存机制才会比较清楚。

　　        •n_jobs:并行设置

　　        •algorithm:kmeans的实现算法，有：’auto’,‘full’,‘elkan’,其中‘full’表示用EM方式实现

### 6.3 朴素贝叶斯

**i.高斯朴素贝叶斯**

　　        •priors:先验概率大小，如果没有给定，模型则根据样本数据自己计算（利用极大似然法）。

　　        对象

　　        •class_prior_:每个样本的概率

　　        •class_count:每个类别的样本数量

　　        •theta_:每个类别中每个特征的均值

　　        •sigma_:每个类别中每个特征的方差

**ii.伯努利朴素贝叶斯**

　　        •alpha:平滑因子，与多项式中的alpha一致。

　　        •binarize:样本特征二值化的阈值，默认是0。如果不输入，则模型会认为所有特征都已经是二值化形式了；如果输入具体的值，则模型会把大于该值的部分归为一类，小于的归为另一类。

　　        •fit_prior:是否去学习类的先验概率，默认是True

　　        •class_prior:各个类别的先验概率，如果没有指定，则模型会根据数据自动学习，每个类别的先验概率相同，等于类标记总个数N分之一。

### 6.4 决策树

•criterion:特征选择的标准，有信息增益和基尼系数两种，使用信息增益的是ID3和C4.5算法（使用信息增益比），使用基尼系数的CART算法，默认是gini系数。

　　        •splitter:特征切分点选择标准，决策树是递归地选择最优切分点，spliter是用来指明在哪个集合上来递归，有“best”和“random”两种参数可以选择，best表示在所有特征上递归，适用于数据集较小的时候，random表示随机选择一部分特征进行递归，适用于数据集较大的时候。

　　        •max_depth:决策树最大深度，决策树模型先对所有数据集进行切分，再在子数据集上继续循环这个切分过程，max_depth可以理解成用来限制这个循环次数。

　　        •min_samples_split:子数据集再切分需要的最小样本量，默认是2，如果子数据样本量小于2时，则不再进行下一步切分。如果数据量较小，使用默认值就可，如果数据量较大，为降低计算量，应该把这个值增大，即限制子数据集的切分次数。

　　        •min_samples_leaf:叶节点（子数据集）最小样本数，如果子数据集中的样本数小于这个值，那么该叶节点和其兄弟节点都会被剪枝（去掉），该值默认为1。

　　        •min_weight_fraction_leaf:在叶节点处的所有输入样本权重总和的最小加权分数，如果不输入则表示所有的叶节点的权重是一致的。

　　        •max_features:特征切分时考虑的最大特征数量，默认是对所有特征进行切分，也可以传入int类型的值，表示具体的特征个数；也可以是浮点数，则表示特征个数的百分比；还可以是sqrt,表示总特征数的平方根；也可以是log2，表示总特征数的log个特征。

　　        •random_state:随机种子的设置，与LR中参数一致。

　　        •max_leaf_nodes:最大叶节点个数，即数据集切分成子数据集的最大个数。

　　        •min_impurity_decrease:切分点不纯度最小减少程度，如果某个结点的不纯度减少小于这个值，那么该切分点就会被移除。

　　        •min_impurity_split:切分点最小不纯度，用来限制数据集的继续切分（决策树的生成），如果某个节点的不纯度（可以理解为分类错误率）小于这个阈值，那么该点的数据将不再进行切分。

　　        •class_weight:权重设置，主要是用于处理不平衡样本，与LR模型中的参数一致，可以自定义类别权重，也可以直接使用balanced参数值进行不平衡样本处理。

　　        •presort:是否进行预排序，默认是False，所谓预排序就是提前对特征进行排序，我们知道，决策树分割数据集的依据是，优先按照信息增益/基尼系数大的特征来进行分割的，涉及的大小就需要比较，如果不进行预排序，则会在每次分割的时候需要重新把所有特征进行计算比较一次，如果进行了预排序以后，则每次分割的时候，只需要拿排名靠前的特征就可以了。

### 6.5 对象/属性

§classes_:分类模型的类别，以字典的形式输出

　　        §feature_importances_:特征重要性，以列表的形式输出每个特征的重要性max_features_:最大特征数

　　        §n_classes_:类别数，与classes_对应，classes_输出具体的类别

　　        §n_features_:特征数，当数据量小时，一般max_features和n_features_相等

　　        §n_outputs_:输出结果数tree_:输出整个决策树,用于生成决策树的可视化

　　        方法

　　        §decision_path(X):返回X的决策路径fit(X,y):在数据集(X,y)上使用决策树模型

　　        §get_params([deep]):获取模型的参数

　　        §predict(X):预测数据值X的标签

　　        §predict_log_proba(X):返回每个类别的概率值的对数

　　        §predict_proba(X):返回每个类别的概率值（有几类就返回几列值）

　　score(X,y):返回给定测试集和对应标签的平均准确率