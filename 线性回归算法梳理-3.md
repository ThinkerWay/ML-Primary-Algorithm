# 线性回归算法梳理-3

1.信息论基础（熵 联合熵 条件熵 信息增益 基尼不纯度）

- 熵表示的是随机变量不确定性的度量。熵越大，随机变量的不确定性也就越大。

- 联合熵和条件熵
  两个随机变量X，Y的联合分布形成联合熵。
  即在已知一个变量的前提下，另一个变量的不确定性。

- 信息增益
  熵值减去条件熵值，代表了在一个条件下，信息复杂度（不确定性）减少的程度。

- 基尼不纯度
  基尼不纯度为这个样本被选中的概率乘以它被分错的概率；可以作为衡量系统混乱程度的标准,值越小，代表分类效果越好，值为0，表示只有一个类别。

  2.决策树的不同分类算法（ID3算法、C4.5、CART分类树）的原理及应用场景

   决策树概念：所谓决策树，顾名思义，就是一种树，一种依托于策略抉择而建立起来的树。在机器学习中，决策树是一种预测模型，代表的是一种对象特征属性与对象目标值之间的一种映射关系。决策树仅有单一输出，如果有多个输出，可以分别建立独立的决策树以处理不同的输出。

  - ID3算法
    原理：
    ID3算法是J. Ross Quinlan于1975提出的一种贪心算法，用来构造决策树。其建立在“奥卡姆剃刀”的基础上，即越是小型的决策树越优于大的决策树。ID3算法中根据特征选择和信息增益评估，每次选择信息增益最大的特征作为分支标准。
    应用场景：
    ID3算法可用于划分标称型数据集，没有剪枝的过程，为了去除过度数据匹配的问题，可通过裁剪合并相邻的无法产生大量信息增益的叶子节点（例如设置信息增益阀值）。

  - C4.5
    原理：C4.5算法是对ID3算法的改进，C4.5克服了ID3的2个缺点：
    用信息增益选择属性时偏向于选择分枝比较多的属性值，即取值多的属性
    不能处理连续属性
    对于离散特征，C4.5算法不直接使用信息增益，而是使用“增益率”（gain ratio）来选择最优的分支标准。
    应用场景：

  - CART分类树
    原理：不同于前两种算法预测结果为分类结果，CART的预测结果为概率值。并且改进了前两种算法中的一个缺点：使用信息增益或信息增益比时，可选值多的特征往往有更高的信息增益。所以在CART树中，不再采用信息增益或信息增益比，而是在做回归时采用平方误差最小化准则，在做分类时采用基尼指数最小化准则。

    3.回归树原理

  cart生成有两个关键点

  - 如何评价最优二分结果
  - 什么时候停止和如何确定叶子节点的值

   cart分类树采用gini系数来对二分结果进行评价，叶子节点的值使用多数表决，那么回归树呢？我们直接看之前的一个数据集（天气与是否出去玩，是否出去玩改成出去玩的时间）

  sunny    hot    high    FALSE    25
  sunny    hot    high    TRUE    30
  overcast    hot    high    FALSE    46
  rainy    mild    high    FALSE    45
  rainy    cool    normal    FALSE    52
  rainy    cool    normal    TRUE    23
  overcast    cool    normal    TRUE    43
  sunny    mild    high    FALSE    35
  sunny    cool    normal    FALSE    38
  rainy    mild    normal    FALSE    46
  sunny    mild    normal    TRUE    48
  overcast    mild    high    TRUE    52
  overcast    hot    normal    FALSE    44
  rainy    mild    high    TRUE    30

  3.决策树防止过拟合手段

  CART剪枝算法从“完全生长”的决策树的底端剪去一些子树，使决策树变小（模型变简单），从而能够对未知数据有更准确的预测。

  预剪枝：在生成决策树之前，针对每个叶结点计算比较验证集精度，选择精度较大的进行划分
  后剪枝：先从训练集生成一棵完整的决策树，然后自底而上地对非叶节点进行考察，若将该节点对应的子树替换为叶节点能带来决策树的泛化性能提升，则将该子树替换为叶节点

  比较：后剪枝决策树通常比预剪枝决策树保留了更多的分支，一般情况下后剪枝决策树的欠拟合风险很小，泛化性能往往优于预剪枝决策树，但后剪枝决策树训练时间比未剪枝决策树和预剪枝决策树多。

  4.模型评估

  **分类树模型**：采用通用的分类模型评估指标

  - Accuracy
  - Precision
  - Recall
  - F1-score
  - ROC曲线和AUC
  - PR曲线

  **回归树模型**：采用通用的回归模型评估指标

  - MSE
  - MAE
  - R^2

  5.sklearn参数详解，Python绘制决策树

  import numpy as np
  import pandas as pd
  import matplotlib.pyplot as plt
  %matplotlib inline

  from sklearn.datasets import load_iris
  from sklearn.model_selection import train_test_split

  from collections import Counter
  import math
  from math import log

  import pprint

  #data

  def create_data():
      iris = load_iris()
      df = pd.DataFrame(iris.data, columns=iris.feature_names)
      df['label'] = iris.target
      df.columns = ['sepal length', 'sepal width', 'petal length', 'petal width', 'label']
      data = np.array(df.iloc[:100, [0, 1, -1]])

  print(data)

  ​    return data[:,:2], data[:,-1]

  X, y = create_data()
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

  from sklearn.tree import DecisionTreeClassifier
  from sklearn.tree import export_graphviz
  import graphviz
  clf = DecisionTreeClassifier()
  clf.fit(X_train, y_train,)
  clf.score(X_test, y_test)
  tree_pic = export_graphviz(clf, out_file="mytree.pdf")
  with open('mytree.pdf') as f:
      dot_graph = f.read()

  graphviz.Source(dot_graph)

  